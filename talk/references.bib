@incollection{NIPS2012_4824,
    title = {ImageNet Classification with Deep Convolutional Neural Networks},
    author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
    booktitle = {Advances in Neural Information Processing Systems 25},
    editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
    pages = {1097--1105},
    year = {2012},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@article{weideman_integral_2017,
	title = {Integral {Curvature} {Representation} and {Matching} {Algorithms} for {Identification} of {Dolphins} and {Whales}},
	url = {http://arxiv.org/abs/1708.07785},
	abstract = {We address the problem of identifying individual cetaceans from images showing the trailing edge of their fins. Given the trailing edge from an unknown individual, we produce a ranking of known individuals from a database. The nicks and notches along the trailing edge define an individual's unique signature. We define a representation based on integral curvature that is robust to changes in viewpoint and pose, and captures the pattern of nicks and notches in a local neighborhood at multiple scales. We explore two ranking methods that use this representation. The first uses a dynamic programming time-warping algorithm to align two representations, and interprets the alignment cost as a measure of similarity. This algorithm also exploits learned spatial weights to downweight matches from regions of unstable curvature. The second interprets the representation as a feature descriptor. Feature keypoints are defined at the local extrema of the representation. Descriptors for the set of known individuals are stored in a tree structure, which allows us to perform queries given the descriptors from an unknown trailing edge. We evaluate the top-k accuracy on two real-world datasets to demonstrate the effectiveness of the curvature representation, achieving top-1 accuracy scores of approximately 95\% and 80\% for bottlenose dolphins and humpback whales, respectively.},
	urldate = {2019-01-21},
	journal = {arXiv:1708.07785 [cs]},
	author = {Weideman, Hendrik J. and Jablons, Zachary M. and Holmberg, Jason and Flynn, Kiirsten and Calambokidis, John and Tyson, Reny B. and Allen, Jason B. and Wells, Randall S. and Hupman, Krista and Urian, Kim and Stewart, Charles V.},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.07785},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1708.07785 PDF:/Users/yvan/Zotero/storage/2NVQ6U7C/Weideman et al. - 2017 - Integral Curvature Representation and Matching Alg.pdf:application/pdf;arXiv.org Snapshot:/Users/yvan/Zotero/storage/YT3LTFG3/1708.html:text/html}
}

@misc{noauthor_reseau_2018,
	title = {Réseau neuronal convolutif},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://fr.wikipedia.org/w/index.php?title=R%C3%A9seau_neuronal_convolutif&oldid=154347780},
	abstract = {En apprentissage automatique, un réseau de neurones convolutifs ou réseau de neurones à convolution (en anglais CNN ou ConvNet pour Convolutional Neural Networks) est un type de réseau de neurones artificiels acycliques (feed-forward), dans lequel le motif de connexion entre les neurones est inspiré par le cortex visuel des animaux. Les neurones de cette région du cerveau sont arrangés de sorte qu'ils correspondent à des régions qui se chevauchent lors du pavage du champ visuel. Leur fonctionnement est inspiré par les processus biologiques, ils consistent en un empilage multicouche de perceptrons, dont le but est de prétraiter de petites quantités d'informations. Les réseaux neuronaux convolutifs ont de larges applications dans la reconnaissance d'image et vidéo, les systèmes de recommandation et le traitement du langage naturel.},
	language = {fr},
	urldate = {2019-01-22},
	journal = {Wikipédia},
	month = nov,
	year = {2018},
	note = {Page Version ID: 154347780},
	file = {Snapshot:/Users/yvan/Zotero/storage/6CQD9IYI/index.html:text/html}
}

@misc{noauthor_tenseur_2018,
	title = {Tenseur},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://fr.wikipedia.org/w/index.php?title=Tenseur&oldid=152725014},
	abstract = {En mathématiques, plus précisément en algèbre multilinéaire et en géométrie différentielle, un tenseur désigne un objet très général, dont la valeur s'exprime dans un espace vectoriel. On peut l'utiliser entre autres pour représenter des applications multilinéaires ou des multivecteurs. On pourrait abusivement considérer qu'un tenseur est une généralisation à n indices du concept de matrice carrée (la matrice possède un indice ligne et un indice colonne — un tenseur peut posséder un nombre arbitraire d'indices inférieurs, covariants, et d'indices supérieurs, contravariants, à ne pas confondre avec des exposants), mais la comparaison s'arrête là car une matrice n'est qu'un simple tableau de nombres qui peut être utilisé pour représenter des objets abstraits, alors que le tenseur est, comme les vecteurs et les applications multilinéaires, un objet abstrait dont les coordonnées changent lorsqu'on passe d'une représentation dans une base donnée à celle dans une autre base.
On peut envisager l'outil tenseur dans 4 types d'utilisation différents :

Le cas simple, où on l'utilise pour ses capacités à représenter des objets algébriques complexes et où on n'a pas besoin des concepts de distances ni d'angles ; on n'introduira pas de produit scalaire, et dans ce cas les coordonnées covariantes représentent des objets de type application linéaire et les coordonnées contravariantes représentent des objets de type (multi-)vecteurs.
Le cas où la base est orthonormée, et où il n'y a pas de différence entre coordonnées covariantes et contravariantes.
Le cas où la base n'est pas orthonormée, et où le produit scalaire est défini par un tenseur métrique. Dans ce cas, le tenseur métrique permet de convertir les coordonnées covariantes en coordonnées contravariantes (et vice versa).
Le cas des espaces courbes de Riemann et plus tard, de la relativité générale, dans lesquels le tenseur métrique est en fait un champ de tenseurs appelé métrique riemannienne (resp Métrique pseudo-riemannienne) et qui dépend donc de la position.Dans tous ces cas, le terme tenseur est souvent utilisé par extension, pour désigner un champ de tenseurs, c'est-à-dire une application qui associe à chaque point d'un espace géométrique un tenseur différent.

En physique, les tenseurs sont utilisés pour décrire et manipuler diverses grandeurs et propriétés physiques comme le champ électrique, la permittivité, les déformations, les contraintes etc.
La première utilisation de la notion et du terme de tenseur s'est faite dans le cadre de la mécanique des milieux continus, en relation avec la nécessité de décrire les contraintes et les déformations subies par les corps étendus, à partir de laquelle fut formalisée la mécanique rationnelle. En particulier, le tenseur des contraintes et le tenseur des déformations sont utilisés dans la science des constructions pour définir l'état de tension et de déformation en tout point d'une structure. Outre la mécanique des fluides et mécanique du solide, les tenseurs sont utilisés dans de nombreux autres domaines de la physique, tels que l'électromagnétisme. Ils sont également largement utilisés en relativité générale, pour décrire rigoureusement l'espace-temps comme variété courbe quadri-dimensionnelle.
Les tenseurs sont également utilisés en géométrie différentielle pour définir sur une variété différentielle les notions géométriques de distance, d'angle et de volume. Cela se fait par le choix d'un tenseur métrique, c'est-à-dire un produit scalaire défini sur l'espace tangent de chaque point. Grâce à ce concept, sont alors définies et étudiées les questions liées à la courbure de la variété. D'autres tenseurs, tels que le tenseur de Riemann et le tenseur de Ricci, sont des outils importants pour cette étude.},
	language = {fr},
	urldate = {2019-01-22},
	journal = {Wikipédia},
	month = oct,
	year = {2018},
	note = {Page Version ID: 152725014},
	file = {Snapshot:/Users/yvan/Zotero/storage/2DP8YCN8/index.html:text/html}
}

@misc{noauthor_approximate_2019,
	title = {Approximate {Nearest} {Neighbors} in {C}++/{Python} optimized for memory usage and loading/saving to disk: spotify/annoy},
	copyright = {Apache-2.0},
	shorttitle = {Approximate {Nearest} {Neighbors} in {C}++/{Python} optimized for memory usage and loading/saving to disk},
	url = {https://github.com/spotify/annoy},
	urldate = {2019-01-22},
	publisher = {Spotify},
	month = jan,
	year = {2019},
	note = {original-date: 2013-04-01T20:29:40Z}
}

@misc{noauthor_image_nodate,
	title = {Image {Preprocessing} - {Keras} {Documentation}},
	url = {https://keras.io/preprocessing/image/#apply_transform},
	urldate = {2019-01-22},
	file = {Image Preprocessing - Keras Documentation:/Users/yvan/Zotero/storage/BZEZGYAE/image.html:text/html}
}

@misc{noauthor_neural_nodate,
	title = {neural network - {How} to interpret "loss" and "accuracy" for a machine learning model},
	url = {https://stackoverflow.com/questions/34518656/how-to-interpret-loss-and-accuracy-for-a-machine-learning-model},
	urldate = {2019-01-22},
	journal = {Stack Overflow},
	file = {Snapshot:/Users/yvan/Zotero/storage/JSRWIKU5/how-to-interpret-loss-and-accuracy-for-a-machine-learning-model.html:text/html}
}